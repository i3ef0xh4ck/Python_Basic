$SPARK_HOME/bin/spark-submit --master local[8] --class com.bizseer.anomaly.carpenter.CLI --conf spark.driver.memory=6g  ./target/carpenter.jar train --log-input file --log-input-path 日志文件路径（每行是一条日志） --model-output file --model-output-path 模型输出路径 --debug --iterative

$SPARK_HOME/bin/spark-submit --master local[8] --class com.bizseer.anomaly.carpenter.CLI --conf spark.driver.memory=6g ./target/carpenter.jar batch parse --log-input file --log-input-path 日志文件路径（和上面一样） --model-input file --model-input-path 模型路径 --log-output file --log-output-path 输出的格式化日志路径 --debug

python -c 'import ujson as json, numpy as np, pandas as pd; input_path = 输出的格式化日志路径, output_path = 输出的曲线csv; F = open(input_path); df = pd.DataFrame([(int(x["timestamp"] / 60000), x["template_id"]) for x in [json.loads(l) for l in F]], columns=["timestamp", "template_id"]) ; F.close(); df = df[df["timestamp"] > 0]; agg = pd.concat([df, pd.DataFrame(np.array(np.meshgrid(np.arange(df["timestamp"].min(), df["timestamp"].max()), df["template_id"].unique())).T.reshape(-1,2), columns=df.columns).astype(dict(timestamp=np.int64))]).groupby(by=[*df2.columns], as_index=False).size().reset_index(); agg.columns = [*agg.columns[:-1]] + ["count"]; agg["count"] -= 1; print(df2.shape, df.shape, agg.shape); agg.to_csv(output_path, index=False)'